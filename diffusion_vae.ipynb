{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c6b6881c",
      "metadata": {
        "id": "c6b6881c"
      },
      "outputs": [],
      "source": [
        "## Importing necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from functools import partial\n",
        "import plotly.graph_objects as go\n",
        "from datetime import datetime\n",
        "import plotly.subplots as ms\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader,Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4aab5bf",
      "metadata": {
        "id": "d4aab5bf"
      },
      "source": [
        "### Data Pre-processing and Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ff0f6461",
      "metadata": {
        "id": "ff0f6461"
      },
      "outputs": [],
      "source": [
        "## Reading data of Apple stock from a comma-separated .txt file into a pandas dataframe and removing the header \n",
        "apple_stock = pd.read_csv('APPL.csv')\n",
        "# apple_stock = apple_stock.drop(columns=[4]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "31551a6c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "31551a6c",
        "outputId": "c8d20ab6-6bb2-4127-c657-1f0c09ef71d5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>High</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>close_ems_0</th>\n",
              "      <th>close_ems_1</th>\n",
              "      <th>close_ems_2</th>\n",
              "      <th>close_ems_3</th>\n",
              "      <th>close_ems_4</th>\n",
              "      <th>close_ems_5</th>\n",
              "      <th>close_ems_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20100129</td>\n",
              "      <td>22.35</td>\n",
              "      <td>21.59</td>\n",
              "      <td>21.70</td>\n",
              "      <td>21.32</td>\n",
              "      <td>5.284370e+08</td>\n",
              "      <td>0.004981</td>\n",
              "      <td>-0.007888</td>\n",
              "      <td>0.029859</td>\n",
              "      <td>0.030317</td>\n",
              "      <td>-0.030210</td>\n",
              "      <td>0.006188</td>\n",
              "      <td>0.021598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20100201</td>\n",
              "      <td>21.77</td>\n",
              "      <td>21.71</td>\n",
              "      <td>21.24</td>\n",
              "      <td>21.00</td>\n",
              "      <td>6.026750e+08</td>\n",
              "      <td>-0.029684</td>\n",
              "      <td>-0.029749</td>\n",
              "      <td>0.009720</td>\n",
              "      <td>0.024786</td>\n",
              "      <td>-0.028706</td>\n",
              "      <td>0.004914</td>\n",
              "      <td>0.024498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20100202</td>\n",
              "      <td>21.96</td>\n",
              "      <td>21.26</td>\n",
              "      <td>21.37</td>\n",
              "      <td>21.26</td>\n",
              "      <td>6.204750e+08</td>\n",
              "      <td>-0.011880</td>\n",
              "      <td>-0.017990</td>\n",
              "      <td>-0.015981</td>\n",
              "      <td>-0.009157</td>\n",
              "      <td>0.011664</td>\n",
              "      <td>-0.015653</td>\n",
              "      <td>0.035750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20100203</td>\n",
              "      <td>22.58</td>\n",
              "      <td>21.53</td>\n",
              "      <td>22.58</td>\n",
              "      <td>21.24</td>\n",
              "      <td>1.214267e+09</td>\n",
              "      <td>0.023553</td>\n",
              "      <td>-0.003843</td>\n",
              "      <td>-0.010330</td>\n",
              "      <td>0.023523</td>\n",
              "      <td>0.029184</td>\n",
              "      <td>-0.018698</td>\n",
              "      <td>0.032221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20100204</td>\n",
              "      <td>22.45</td>\n",
              "      <td>22.33</td>\n",
              "      <td>22.20</td>\n",
              "      <td>22.01</td>\n",
              "      <td>6.752830e+08</td>\n",
              "      <td>-0.024795</td>\n",
              "      <td>-0.033569</td>\n",
              "      <td>-0.006393</td>\n",
              "      <td>-0.023653</td>\n",
              "      <td>-0.019699</td>\n",
              "      <td>-0.020172</td>\n",
              "      <td>-0.003472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2135</th>\n",
              "      <td>20181224</td>\n",
              "      <td>9.45</td>\n",
              "      <td>9.40</td>\n",
              "      <td>9.42</td>\n",
              "      <td>9.31</td>\n",
              "      <td>4.771869e+08</td>\n",
              "      <td>0.002830</td>\n",
              "      <td>-0.032855</td>\n",
              "      <td>-0.004628</td>\n",
              "      <td>0.012152</td>\n",
              "      <td>0.015378</td>\n",
              "      <td>-0.010386</td>\n",
              "      <td>-0.023502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2136</th>\n",
              "      <td>20181225</td>\n",
              "      <td>9.43</td>\n",
              "      <td>9.29</td>\n",
              "      <td>9.34</td>\n",
              "      <td>9.21</td>\n",
              "      <td>5.452356e+08</td>\n",
              "      <td>-0.003459</td>\n",
              "      <td>0.002878</td>\n",
              "      <td>-0.001050</td>\n",
              "      <td>0.021730</td>\n",
              "      <td>-0.008739</td>\n",
              "      <td>0.003577</td>\n",
              "      <td>0.022280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2137</th>\n",
              "      <td>20181226</td>\n",
              "      <td>9.42</td>\n",
              "      <td>9.35</td>\n",
              "      <td>9.30</td>\n",
              "      <td>9.27</td>\n",
              "      <td>3.932151e+08</td>\n",
              "      <td>-0.009513</td>\n",
              "      <td>0.017691</td>\n",
              "      <td>0.035619</td>\n",
              "      <td>-0.011251</td>\n",
              "      <td>-0.002966</td>\n",
              "      <td>-0.022544</td>\n",
              "      <td>0.019264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2138</th>\n",
              "      <td>20181227</td>\n",
              "      <td>9.49</td>\n",
              "      <td>9.45</td>\n",
              "      <td>9.28</td>\n",
              "      <td>9.28</td>\n",
              "      <td>5.863437e+08</td>\n",
              "      <td>-0.031788</td>\n",
              "      <td>0.007745</td>\n",
              "      <td>0.003324</td>\n",
              "      <td>0.014035</td>\n",
              "      <td>0.022011</td>\n",
              "      <td>-0.023579</td>\n",
              "      <td>0.023065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2139</th>\n",
              "      <td>20181228</td>\n",
              "      <td>9.46</td>\n",
              "      <td>9.31</td>\n",
              "      <td>9.38</td>\n",
              "      <td>9.31</td>\n",
              "      <td>5.415710e+08</td>\n",
              "      <td>0.025635</td>\n",
              "      <td>-0.000558</td>\n",
              "      <td>-0.013828</td>\n",
              "      <td>0.020227</td>\n",
              "      <td>0.028090</td>\n",
              "      <td>-0.013999</td>\n",
              "      <td>0.027782</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2140 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Date   High   Open  Close    Low        Volume  close_ems_0  \\\n",
              "0     20100129  22.35  21.59  21.70  21.32  5.284370e+08     0.004981   \n",
              "1     20100201  21.77  21.71  21.24  21.00  6.026750e+08    -0.029684   \n",
              "2     20100202  21.96  21.26  21.37  21.26  6.204750e+08    -0.011880   \n",
              "3     20100203  22.58  21.53  22.58  21.24  1.214267e+09     0.023553   \n",
              "4     20100204  22.45  22.33  22.20  22.01  6.752830e+08    -0.024795   \n",
              "...        ...    ...    ...    ...    ...           ...          ...   \n",
              "2135  20181224   9.45   9.40   9.42   9.31  4.771869e+08     0.002830   \n",
              "2136  20181225   9.43   9.29   9.34   9.21  5.452356e+08    -0.003459   \n",
              "2137  20181226   9.42   9.35   9.30   9.27  3.932151e+08    -0.009513   \n",
              "2138  20181227   9.49   9.45   9.28   9.28  5.863437e+08    -0.031788   \n",
              "2139  20181228   9.46   9.31   9.38   9.31  5.415710e+08     0.025635   \n",
              "\n",
              "      close_ems_1  close_ems_2  close_ems_3  close_ems_4  close_ems_5  \\\n",
              "0       -0.007888     0.029859     0.030317    -0.030210     0.006188   \n",
              "1       -0.029749     0.009720     0.024786    -0.028706     0.004914   \n",
              "2       -0.017990    -0.015981    -0.009157     0.011664    -0.015653   \n",
              "3       -0.003843    -0.010330     0.023523     0.029184    -0.018698   \n",
              "4       -0.033569    -0.006393    -0.023653    -0.019699    -0.020172   \n",
              "...           ...          ...          ...          ...          ...   \n",
              "2135    -0.032855    -0.004628     0.012152     0.015378    -0.010386   \n",
              "2136     0.002878    -0.001050     0.021730    -0.008739     0.003577   \n",
              "2137     0.017691     0.035619    -0.011251    -0.002966    -0.022544   \n",
              "2138     0.007745     0.003324     0.014035     0.022011    -0.023579   \n",
              "2139    -0.000558    -0.013828     0.020227     0.028090    -0.013999   \n",
              "\n",
              "      close_ems_6  \n",
              "0        0.021598  \n",
              "1        0.024498  \n",
              "2        0.035750  \n",
              "3        0.032221  \n",
              "4       -0.003472  \n",
              "...           ...  \n",
              "2135    -0.023502  \n",
              "2136     0.022280  \n",
              "2137     0.019264  \n",
              "2138     0.023065  \n",
              "2139     0.027782  \n",
              "\n",
              "[2140 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Separating the date and time using str.split() for better interpretation\n",
        "# apple_stock.columns = ['Open','High','Low','Close','Volume', 'Date']\n",
        "# apple_stock[['Date','Time']] = apple_stock['Date and Time'].str.split(' ',1,expand = True)\n",
        "## Dropping the original column and retaining the newly created columns\n",
        "# apple_stock = apple_stock.drop('Date and Time',axis=1)\n",
        "## Displaying the modified dataframe\n",
        "display(apple_stock)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0fad5d5a",
      "metadata": {
        "id": "0fad5d5a"
      },
      "outputs": [],
      "source": [
        "# ## As we are interested in predicting stock prices for the next 5 days we only require day-wise data and not minute wise data\n",
        "# # Thus, we group the data by 'Date' setting values for all the columns appropriately\n",
        "# apple_stock = apple_stock.groupby('Date').agg({\n",
        "#         'Close' : 'last',\n",
        "#         'Open' : 'first',\n",
        "#         'High' : 'max',\n",
        "#         'Low' : 'min',\n",
        "#         'Volume' : 'sum'\n",
        "# }).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "Wf9PhDGmckqT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wf9PhDGmckqT",
        "outputId": "370accb4-9bd7-47dd-c49e-a10eb1bb65f0"
      },
      "outputs": [],
      "source": [
        "# apple_stock.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ba04ca24",
      "metadata": {
        "id": "ba04ca24"
      },
      "outputs": [],
      "source": [
        "## Creating features/indicators that are suitable for financial data and aid in stock price predictions as they help in determining the trends\n",
        "## Creating a separate column for stochastic oscillator given by (C-L)/(H-L)\n",
        "apple_stock['Stochastic Oscillator'] = (apple_stock['Close'] - apple_stock['Low'])/(apple_stock['High']-apple_stock['Low'])\n",
        "## Defining absolute returns as c_t - c_t-1\n",
        "apple_stock['Absolute Returns'] = apple_stock['Close'] - apple_stock['Close'].shift(1)\n",
        "## Normalizing opening, closing, high and low prices using the previous days' Close\n",
        "apple_stock['Close Normalized'] = apple_stock['Close']/apple_stock['Close'].shift(1)\n",
        "apple_stock['Open Normalized'] = apple_stock['Open']/apple_stock['Close'].shift(1)\n",
        "apple_stock['High Value Normalized'] = apple_stock['High']/apple_stock['Close'].shift(1)\n",
        "apple_stock['Low Value Normalized'] = apple_stock['Low']/apple_stock['Close'].shift(1)\n",
        "## Normalizing volume of stocks traded by a 5-day rolling mean of the volume\n",
        "apple_stock['Volume Normalized'] =  apple_stock['Volume']/apple_stock['Volume'].shift(1).rolling(window=5).mean()\n",
        "## Defining volatility by the variance of volume of stocks traded over a window of 9 days\n",
        "apple_stock['Volatility'] = apple_stock['Volume'].rolling(window=9).var()/1e16\n",
        "## Removing unnecessary rows and resetting the index\n",
        "apple_stock.dropna(inplace = True)\n",
        "apple_stock.reset_index(drop = True, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "41ff15f6",
      "metadata": {
        "id": "41ff15f6"
      },
      "outputs": [],
      "source": [
        "# ## Defining MACD () as  5 day EMA - 9 day EMA\n",
        "# ## Defining the periods over which EMA is to be calculated\n",
        "# period1 = 9\n",
        "# period2 = 5\n",
        "# # Calculate the smoothing factor (alpha)\n",
        "# alpha1 = 2 / (period1 + 1)\n",
        "# alpha2 = 2/(period2 +1 )\n",
        "# # Calculate 9-day EMA and 5-day EMA using the pandas `ewm` method\n",
        "# EMA9day = apple_stock['Close'].ewm(span=period1, adjust=False).mean()\n",
        "# EMA5day = apple_stock['Close'].ewm(span=period2, adjust=False).mean()\n",
        "# apple_stock['MACD'] = EMA5day - EMA9day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "194d1069",
      "metadata": {
        "id": "194d1069"
      },
      "outputs": [],
      "source": [
        "# ## Defining Gains/Losses for a particular day depending on whether O>C or C>O\n",
        "# apple_stock['Gains'] = (apple_stock['Close'] - apple_stock['Open'])*100/apple_stock['Open']\n",
        "# apple_stock['Gains'] = apple_stock['Gains'].apply(lambda x: max(0, x))\n",
        "# apple_stock['Losses'] = (apple_stock['Open'] - apple_stock['Close'])*100/apple_stock['Open']\n",
        "# apple_stock['Losses'] = apple_stock['Losses'].apply(lambda x: max(0, x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5a37685f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a37685f",
        "outputId": "98e10e2a-ba76-4158-b37a-f45de286b711"
      },
      "outputs": [],
      "source": [
        "# ## Computing the Relative Strength Index (RSI) using the gains/losses which is dependent on average gains and average losses\n",
        "# rsi_data = []\n",
        "# ## Defining epsilon to avoid nan related issues (small value)\n",
        "# eps = 1e-8\n",
        "# for i in range(9,len(apple_stock)):\n",
        "#     ## Calculating the RSI gains for the last 9 days\n",
        "#     rsi_gains = np.array(apple_stock.iloc[i-9:i]['Gains'])\n",
        "#     ## Calculating average positive gains over the last 9 days\n",
        "#     average_gains = np.mean(rsi_gains[rsi_gains > 0])\n",
        "#     average_gains = 0 if np.isnan(average_gains) else average_gains\n",
        "#     ## Calculating RSI losses for the last 9 days\n",
        "#     rsi_losses = np.array(apple_stock.iloc[i-9:i]['Losses'])\n",
        "#     average_losses = np.mean(rsi_losses[rsi_losses > 0])\n",
        "#     average_losses = 0 if np.isnan(average_losses) else average_losses\n",
        "#     ## Computing the RSI index\n",
        "#     den = 1+ average_gains/(average_losses + eps)\n",
        "#     rsi_data.append(100-(100/den))\n",
        "\n",
        "# #Removing the first 9 rows, due to rolling mean\n",
        "# apple_stock = apple_stock.iloc[9:]\n",
        "# apple_stock['RSI'] = rsi_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9098c56a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9098c56a",
        "outputId": "22667ffd-3a07-4bbf-d7f3-70ed5ab003dc"
      },
      "outputs": [],
      "source": [
        "# ## Using periodic features that help to discover repetitive patterns or cycles within the data\n",
        "# ## These cycles aid in predicting future price movements\n",
        "# apple_stock.reset_index(inplace = True, drop = True)\n",
        "# apple_stock['Sine'] = np.sin(2*np.pi/20*pd.DatetimeIndex(data = apple_stock['Date'], yearfirst = True).day)\n",
        "# apple_stock['Cosine'] = np.cos(2*np.pi/20*pd.DatetimeIndex(data = apple_stock['Date'], yearfirst = True).day)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "3f55cb91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "3f55cb91",
        "outputId": "9a225de8-75f2-4f2a-dab1-1951809763f3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>High</th>\n",
              "      <th>Open</th>\n",
              "      <th>Close</th>\n",
              "      <th>Low</th>\n",
              "      <th>Volume</th>\n",
              "      <th>close_ems_0</th>\n",
              "      <th>close_ems_1</th>\n",
              "      <th>close_ems_2</th>\n",
              "      <th>close_ems_3</th>\n",
              "      <th>...</th>\n",
              "      <th>close_ems_5</th>\n",
              "      <th>close_ems_6</th>\n",
              "      <th>Stochastic Oscillator</th>\n",
              "      <th>Absolute Returns</th>\n",
              "      <th>Close Normalized</th>\n",
              "      <th>Open Normalized</th>\n",
              "      <th>High Value Normalized</th>\n",
              "      <th>Low Value Normalized</th>\n",
              "      <th>Volume Normalized</th>\n",
              "      <th>Volatility</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20100210</td>\n",
              "      <td>22.38</td>\n",
              "      <td>22.20</td>\n",
              "      <td>22.28</td>\n",
              "      <td>22.02</td>\n",
              "      <td>564177024.0</td>\n",
              "      <td>-0.011256</td>\n",
              "      <td>-0.005505</td>\n",
              "      <td>0.019490</td>\n",
              "      <td>0.005371</td>\n",
              "      <td>...</td>\n",
              "      <td>0.027786</td>\n",
              "      <td>-0.013485</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>0.28</td>\n",
              "      <td>1.012727</td>\n",
              "      <td>1.009091</td>\n",
              "      <td>1.017273</td>\n",
              "      <td>1.000909</td>\n",
              "      <td>0.810291</td>\n",
              "      <td>4.986343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20100211</td>\n",
              "      <td>22.35</td>\n",
              "      <td>22.21</td>\n",
              "      <td>22.11</td>\n",
              "      <td>22.08</td>\n",
              "      <td>308844000.0</td>\n",
              "      <td>-0.009037</td>\n",
              "      <td>0.016737</td>\n",
              "      <td>0.033113</td>\n",
              "      <td>-0.012798</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.019664</td>\n",
              "      <td>0.016362</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>0.992370</td>\n",
              "      <td>0.996858</td>\n",
              "      <td>1.003142</td>\n",
              "      <td>0.991023</td>\n",
              "      <td>0.545423</td>\n",
              "      <td>6.157216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20100212</td>\n",
              "      <td>22.60</td>\n",
              "      <td>22.17</td>\n",
              "      <td>22.45</td>\n",
              "      <td>22.17</td>\n",
              "      <td>422488000.0</td>\n",
              "      <td>0.025628</td>\n",
              "      <td>-0.002779</td>\n",
              "      <td>-0.013408</td>\n",
              "      <td>0.019327</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.017680</td>\n",
              "      <td>0.029322</td>\n",
              "      <td>0.651163</td>\n",
              "      <td>0.34</td>\n",
              "      <td>1.015378</td>\n",
              "      <td>1.002714</td>\n",
              "      <td>1.022162</td>\n",
              "      <td>1.002714</td>\n",
              "      <td>0.857046</td>\n",
              "      <td>6.594753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20100222</td>\n",
              "      <td>22.45</td>\n",
              "      <td>22.31</td>\n",
              "      <td>22.10</td>\n",
              "      <td>22.08</td>\n",
              "      <td>367932992.0</td>\n",
              "      <td>0.022317</td>\n",
              "      <td>-0.017556</td>\n",
              "      <td>0.027884</td>\n",
              "      <td>0.017637</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010337</td>\n",
              "      <td>0.020838</td>\n",
              "      <td>0.054054</td>\n",
              "      <td>-0.35</td>\n",
              "      <td>0.984410</td>\n",
              "      <td>0.993764</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.983519</td>\n",
              "      <td>0.804835</td>\n",
              "      <td>7.172232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20100223</td>\n",
              "      <td>22.05</td>\n",
              "      <td>22.03</td>\n",
              "      <td>21.55</td>\n",
              "      <td>21.40</td>\n",
              "      <td>513376992.0</td>\n",
              "      <td>-0.006957</td>\n",
              "      <td>-0.016416</td>\n",
              "      <td>-0.019102</td>\n",
              "      <td>-0.010108</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.015142</td>\n",
              "      <td>0.029848</td>\n",
              "      <td>0.230769</td>\n",
              "      <td>-0.55</td>\n",
              "      <td>0.975113</td>\n",
              "      <td>0.996833</td>\n",
              "      <td>0.997738</td>\n",
              "      <td>0.968326</td>\n",
              "      <td>1.163296</td>\n",
              "      <td>1.370266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2085</th>\n",
              "      <td>20181224</td>\n",
              "      <td>9.45</td>\n",
              "      <td>9.40</td>\n",
              "      <td>9.42</td>\n",
              "      <td>9.31</td>\n",
              "      <td>477186912.0</td>\n",
              "      <td>0.002830</td>\n",
              "      <td>-0.032855</td>\n",
              "      <td>-0.004628</td>\n",
              "      <td>0.012152</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.010386</td>\n",
              "      <td>-0.023502</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.996825</td>\n",
              "      <td>0.994709</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.985185</td>\n",
              "      <td>0.655370</td>\n",
              "      <td>4.364597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2086</th>\n",
              "      <td>20181225</td>\n",
              "      <td>9.43</td>\n",
              "      <td>9.29</td>\n",
              "      <td>9.34</td>\n",
              "      <td>9.21</td>\n",
              "      <td>545235584.0</td>\n",
              "      <td>-0.003459</td>\n",
              "      <td>0.002878</td>\n",
              "      <td>-0.001050</td>\n",
              "      <td>0.021730</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003577</td>\n",
              "      <td>0.022280</td>\n",
              "      <td>0.590909</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>0.991507</td>\n",
              "      <td>0.986200</td>\n",
              "      <td>1.001062</td>\n",
              "      <td>0.977707</td>\n",
              "      <td>0.771611</td>\n",
              "      <td>3.518071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2087</th>\n",
              "      <td>20181226</td>\n",
              "      <td>9.42</td>\n",
              "      <td>9.35</td>\n",
              "      <td>9.30</td>\n",
              "      <td>9.27</td>\n",
              "      <td>393215136.0</td>\n",
              "      <td>-0.009513</td>\n",
              "      <td>0.017691</td>\n",
              "      <td>0.035619</td>\n",
              "      <td>-0.011251</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.022544</td>\n",
              "      <td>0.019264</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.995717</td>\n",
              "      <td>1.001071</td>\n",
              "      <td>1.008565</td>\n",
              "      <td>0.992505</td>\n",
              "      <td>0.556776</td>\n",
              "      <td>3.932607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2088</th>\n",
              "      <td>20181227</td>\n",
              "      <td>9.49</td>\n",
              "      <td>9.45</td>\n",
              "      <td>9.28</td>\n",
              "      <td>9.28</td>\n",
              "      <td>586343744.0</td>\n",
              "      <td>-0.031788</td>\n",
              "      <td>0.007745</td>\n",
              "      <td>0.003324</td>\n",
              "      <td>0.014035</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.023579</td>\n",
              "      <td>0.023065</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.997849</td>\n",
              "      <td>1.016129</td>\n",
              "      <td>1.020430</td>\n",
              "      <td>0.997849</td>\n",
              "      <td>0.881905</td>\n",
              "      <td>3.860181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2089</th>\n",
              "      <td>20181228</td>\n",
              "      <td>9.46</td>\n",
              "      <td>9.31</td>\n",
              "      <td>9.38</td>\n",
              "      <td>9.31</td>\n",
              "      <td>541571008.0</td>\n",
              "      <td>0.025635</td>\n",
              "      <td>-0.000558</td>\n",
              "      <td>-0.013828</td>\n",
              "      <td>0.020227</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.013999</td>\n",
              "      <td>0.027782</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.10</td>\n",
              "      <td>1.010776</td>\n",
              "      <td>1.003233</td>\n",
              "      <td>1.019397</td>\n",
              "      <td>1.003233</td>\n",
              "      <td>0.919025</td>\n",
              "      <td>3.926387</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2090 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Date   High   Open  Close    Low       Volume  close_ems_0  \\\n",
              "0     20100210  22.38  22.20  22.28  22.02  564177024.0    -0.011256   \n",
              "1     20100211  22.35  22.21  22.11  22.08  308844000.0    -0.009037   \n",
              "2     20100212  22.60  22.17  22.45  22.17  422488000.0     0.025628   \n",
              "3     20100222  22.45  22.31  22.10  22.08  367932992.0     0.022317   \n",
              "4     20100223  22.05  22.03  21.55  21.40  513376992.0    -0.006957   \n",
              "...        ...    ...    ...    ...    ...          ...          ...   \n",
              "2085  20181224   9.45   9.40   9.42   9.31  477186912.0     0.002830   \n",
              "2086  20181225   9.43   9.29   9.34   9.21  545235584.0    -0.003459   \n",
              "2087  20181226   9.42   9.35   9.30   9.27  393215136.0    -0.009513   \n",
              "2088  20181227   9.49   9.45   9.28   9.28  586343744.0    -0.031788   \n",
              "2089  20181228   9.46   9.31   9.38   9.31  541571008.0     0.025635   \n",
              "\n",
              "      close_ems_1  close_ems_2  close_ems_3  ...  close_ems_5  close_ems_6  \\\n",
              "0       -0.005505     0.019490     0.005371  ...     0.027786    -0.013485   \n",
              "1        0.016737     0.033113    -0.012798  ...    -0.019664     0.016362   \n",
              "2       -0.002779    -0.013408     0.019327  ...    -0.017680     0.029322   \n",
              "3       -0.017556     0.027884     0.017637  ...     0.010337     0.020838   \n",
              "4       -0.016416    -0.019102    -0.010108  ...    -0.015142     0.029848   \n",
              "...           ...          ...          ...  ...          ...          ...   \n",
              "2085    -0.032855    -0.004628     0.012152  ...    -0.010386    -0.023502   \n",
              "2086     0.002878    -0.001050     0.021730  ...     0.003577     0.022280   \n",
              "2087     0.017691     0.035619    -0.011251  ...    -0.022544     0.019264   \n",
              "2088     0.007745     0.003324     0.014035  ...    -0.023579     0.023065   \n",
              "2089    -0.000558    -0.013828     0.020227  ...    -0.013999     0.027782   \n",
              "\n",
              "      Stochastic Oscillator  Absolute Returns  Close Normalized  \\\n",
              "0                  0.722222              0.28          1.012727   \n",
              "1                  0.111111             -0.17          0.992370   \n",
              "2                  0.651163              0.34          1.015378   \n",
              "3                  0.054054             -0.35          0.984410   \n",
              "4                  0.230769             -0.55          0.975113   \n",
              "...                     ...               ...               ...   \n",
              "2085               0.785714             -0.03          0.996825   \n",
              "2086               0.590909             -0.08          0.991507   \n",
              "2087               0.200000             -0.04          0.995717   \n",
              "2088               0.000000             -0.02          0.997849   \n",
              "2089               0.466667              0.10          1.010776   \n",
              "\n",
              "      Open Normalized  High Value Normalized  Low Value Normalized  \\\n",
              "0            1.009091               1.017273              1.000909   \n",
              "1            0.996858               1.003142              0.991023   \n",
              "2            1.002714               1.022162              1.002714   \n",
              "3            0.993764               1.000000              0.983519   \n",
              "4            0.996833               0.997738              0.968326   \n",
              "...               ...                    ...                   ...   \n",
              "2085         0.994709               1.000000              0.985185   \n",
              "2086         0.986200               1.001062              0.977707   \n",
              "2087         1.001071               1.008565              0.992505   \n",
              "2088         1.016129               1.020430              0.997849   \n",
              "2089         1.003233               1.019397              1.003233   \n",
              "\n",
              "      Volume Normalized  Volatility  \n",
              "0              0.810291    4.986343  \n",
              "1              0.545423    6.157216  \n",
              "2              0.857046    6.594753  \n",
              "3              0.804835    7.172232  \n",
              "4              1.163296    1.370266  \n",
              "...                 ...         ...  \n",
              "2085           0.655370    4.364597  \n",
              "2086           0.771611    3.518071  \n",
              "2087           0.556776    3.932607  \n",
              "2088           0.881905    3.860181  \n",
              "2089           0.919025    3.926387  \n",
              "\n",
              "[2090 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Displaying the final dataset with all features\n",
        "display(apple_stock)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d212fe9",
      "metadata": {
        "id": "4d212fe9"
      },
      "source": [
        "### Plots"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5bd34eb",
      "metadata": {
        "id": "e5bd34eb"
      },
      "source": [
        "### Sequencer and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "01246fef",
      "metadata": {
        "id": "01246fef"
      },
      "outputs": [],
      "source": [
        "## Defining a Sequencer which converts the data into a format suitable for training\n",
        "class StockDataset(Dataset):\n",
        "    def __init__(self,data,sequence_length,prediction_length):\n",
        "        ## Initializing the dataframe and the window length i.e. number of previous days which will be used at a time to predict\n",
        "        ## today's Close\n",
        "        self.data = data\n",
        "        self.sequence_length = sequence_length\n",
        "        self.prediction_length = prediction_length\n",
        "\n",
        "    def __len__(self):\n",
        "        ## As it picks indices randomly from [0,len], we keep len =  len(df) - seq_len which denotes the last index which can be\n",
        "        ## used to create a batch as we need seq_len rows ahead of it\n",
        "        return len(self.data) - self.sequence_length - self.prediction_length\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        ## Slicing the dataframe from input index to input index + seq_len to get the input data\n",
        "        input_data = self.data[index : index + self.sequence_length]\n",
        "        input_list = input_data.values.tolist()\n",
        "        input = torch.Tensor(input_list)\n",
        "\n",
        "        ## Returning the Closes of next day as the output for each day in the input\n",
        "        ## Converting both the input and output to tensors before returning\n",
        "        output = self.data.loc[index + self.sequence_length : index + self.sequence_length + self.prediction_length-1, 'Close Normalized'].values.tolist()\n",
        "        output = torch.Tensor(output)\n",
        "\n",
        "        return input,output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "XEDgsn7gcfPW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEDgsn7gcfPW",
        "outputId": "5ed3727b-ba25-424f-e4ba-2135ef81983d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Date', 'High', 'Open', 'Close', 'Low', 'Volume', 'close_ems_0',\n",
              "       'close_ems_1', 'close_ems_2', 'close_ems_3', 'close_ems_4',\n",
              "       'close_ems_5', 'close_ems_6', 'Stochastic Oscillator',\n",
              "       'Absolute Returns', 'Close Normalized', 'Open Normalized',\n",
              "       'High Value Normalized', 'Low Value Normalized', 'Volume Normalized',\n",
              "       'Volatility'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "apple_stock.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4f64794f",
      "metadata": {
        "id": "4f64794f"
      },
      "outputs": [],
      "source": [
        "# input_features = ['Close Normalized', 'Open Normalized','High Value Normalized','Low Value Normalized','Volume Normalized']\n",
        "# input_features = ['Close Normalized', 'Open Normalized','High Value Normalized','Low Value Normalized','Volume Normalized',\n",
        "#                  'Stochastic Oscillator', 'Absolute Returns','Volatility','MACD','RSI','Sine','Cosine']\n",
        "input_features = ['Close Normalized', 'Open Normalized','High Value Normalized','Low Value Normalized','Volume Normalized',\n",
        "                 'close_ems_0', 'close_ems_1','close_ems_2','close_ems_3','close_ems_4','close_ems_5','close_ems_6']\n",
        "df_app = apple_stock[input_features].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "J-T5v0KSd4ol",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "J-T5v0KSd4ol",
        "outputId": "5d572247-eb56-44b3-de4d-70984c213a58"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close Normalized</th>\n",
              "      <th>Open Normalized</th>\n",
              "      <th>High Value Normalized</th>\n",
              "      <th>Low Value Normalized</th>\n",
              "      <th>Volume Normalized</th>\n",
              "      <th>close_ems_0</th>\n",
              "      <th>close_ems_1</th>\n",
              "      <th>close_ems_2</th>\n",
              "      <th>close_ems_3</th>\n",
              "      <th>close_ems_4</th>\n",
              "      <th>close_ems_5</th>\n",
              "      <th>close_ems_6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.012727</td>\n",
              "      <td>1.009091</td>\n",
              "      <td>1.017273</td>\n",
              "      <td>1.000909</td>\n",
              "      <td>0.810291</td>\n",
              "      <td>-0.011256</td>\n",
              "      <td>-0.005505</td>\n",
              "      <td>0.019490</td>\n",
              "      <td>0.005371</td>\n",
              "      <td>-0.002313</td>\n",
              "      <td>0.027786</td>\n",
              "      <td>-0.013485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.992370</td>\n",
              "      <td>0.996858</td>\n",
              "      <td>1.003142</td>\n",
              "      <td>0.991023</td>\n",
              "      <td>0.545423</td>\n",
              "      <td>-0.009037</td>\n",
              "      <td>0.016737</td>\n",
              "      <td>0.033113</td>\n",
              "      <td>-0.012798</td>\n",
              "      <td>-0.002228</td>\n",
              "      <td>-0.019664</td>\n",
              "      <td>0.016362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.015378</td>\n",
              "      <td>1.002714</td>\n",
              "      <td>1.022162</td>\n",
              "      <td>1.002714</td>\n",
              "      <td>0.857046</td>\n",
              "      <td>0.025628</td>\n",
              "      <td>-0.002779</td>\n",
              "      <td>-0.013408</td>\n",
              "      <td>0.019327</td>\n",
              "      <td>0.031958</td>\n",
              "      <td>-0.017680</td>\n",
              "      <td>0.029322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.984410</td>\n",
              "      <td>0.993764</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.983519</td>\n",
              "      <td>0.804835</td>\n",
              "      <td>0.022317</td>\n",
              "      <td>-0.017556</td>\n",
              "      <td>0.027884</td>\n",
              "      <td>0.017637</td>\n",
              "      <td>-0.021207</td>\n",
              "      <td>0.010337</td>\n",
              "      <td>0.020838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.975113</td>\n",
              "      <td>0.996833</td>\n",
              "      <td>0.997738</td>\n",
              "      <td>0.968326</td>\n",
              "      <td>1.163296</td>\n",
              "      <td>-0.006957</td>\n",
              "      <td>-0.016416</td>\n",
              "      <td>-0.019102</td>\n",
              "      <td>-0.010108</td>\n",
              "      <td>0.013027</td>\n",
              "      <td>-0.015142</td>\n",
              "      <td>0.029848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2085</th>\n",
              "      <td>0.996825</td>\n",
              "      <td>0.994709</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.985185</td>\n",
              "      <td>0.655370</td>\n",
              "      <td>0.002830</td>\n",
              "      <td>-0.032855</td>\n",
              "      <td>-0.004628</td>\n",
              "      <td>0.012152</td>\n",
              "      <td>0.015378</td>\n",
              "      <td>-0.010386</td>\n",
              "      <td>-0.023502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2086</th>\n",
              "      <td>0.991507</td>\n",
              "      <td>0.986200</td>\n",
              "      <td>1.001062</td>\n",
              "      <td>0.977707</td>\n",
              "      <td>0.771611</td>\n",
              "      <td>-0.003459</td>\n",
              "      <td>0.002878</td>\n",
              "      <td>-0.001050</td>\n",
              "      <td>0.021730</td>\n",
              "      <td>-0.008739</td>\n",
              "      <td>0.003577</td>\n",
              "      <td>0.022280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2087</th>\n",
              "      <td>0.995717</td>\n",
              "      <td>1.001071</td>\n",
              "      <td>1.008565</td>\n",
              "      <td>0.992505</td>\n",
              "      <td>0.556776</td>\n",
              "      <td>-0.009513</td>\n",
              "      <td>0.017691</td>\n",
              "      <td>0.035619</td>\n",
              "      <td>-0.011251</td>\n",
              "      <td>-0.002966</td>\n",
              "      <td>-0.022544</td>\n",
              "      <td>0.019264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2088</th>\n",
              "      <td>0.997849</td>\n",
              "      <td>1.016129</td>\n",
              "      <td>1.020430</td>\n",
              "      <td>0.997849</td>\n",
              "      <td>0.881905</td>\n",
              "      <td>-0.031788</td>\n",
              "      <td>0.007745</td>\n",
              "      <td>0.003324</td>\n",
              "      <td>0.014035</td>\n",
              "      <td>0.022011</td>\n",
              "      <td>-0.023579</td>\n",
              "      <td>0.023065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2089</th>\n",
              "      <td>1.010776</td>\n",
              "      <td>1.003233</td>\n",
              "      <td>1.019397</td>\n",
              "      <td>1.003233</td>\n",
              "      <td>0.919025</td>\n",
              "      <td>0.025635</td>\n",
              "      <td>-0.000558</td>\n",
              "      <td>-0.013828</td>\n",
              "      <td>0.020227</td>\n",
              "      <td>0.028090</td>\n",
              "      <td>-0.013999</td>\n",
              "      <td>0.027782</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2090 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Close Normalized  Open Normalized  High Value Normalized  \\\n",
              "0             1.012727         1.009091               1.017273   \n",
              "1             0.992370         0.996858               1.003142   \n",
              "2             1.015378         1.002714               1.022162   \n",
              "3             0.984410         0.993764               1.000000   \n",
              "4             0.975113         0.996833               0.997738   \n",
              "...                ...              ...                    ...   \n",
              "2085          0.996825         0.994709               1.000000   \n",
              "2086          0.991507         0.986200               1.001062   \n",
              "2087          0.995717         1.001071               1.008565   \n",
              "2088          0.997849         1.016129               1.020430   \n",
              "2089          1.010776         1.003233               1.019397   \n",
              "\n",
              "      Low Value Normalized  Volume Normalized  close_ems_0  close_ems_1  \\\n",
              "0                 1.000909           0.810291    -0.011256    -0.005505   \n",
              "1                 0.991023           0.545423    -0.009037     0.016737   \n",
              "2                 1.002714           0.857046     0.025628    -0.002779   \n",
              "3                 0.983519           0.804835     0.022317    -0.017556   \n",
              "4                 0.968326           1.163296    -0.006957    -0.016416   \n",
              "...                    ...                ...          ...          ...   \n",
              "2085              0.985185           0.655370     0.002830    -0.032855   \n",
              "2086              0.977707           0.771611    -0.003459     0.002878   \n",
              "2087              0.992505           0.556776    -0.009513     0.017691   \n",
              "2088              0.997849           0.881905    -0.031788     0.007745   \n",
              "2089              1.003233           0.919025     0.025635    -0.000558   \n",
              "\n",
              "      close_ems_2  close_ems_3  close_ems_4  close_ems_5  close_ems_6  \n",
              "0        0.019490     0.005371    -0.002313     0.027786    -0.013485  \n",
              "1        0.033113    -0.012798    -0.002228    -0.019664     0.016362  \n",
              "2       -0.013408     0.019327     0.031958    -0.017680     0.029322  \n",
              "3        0.027884     0.017637    -0.021207     0.010337     0.020838  \n",
              "4       -0.019102    -0.010108     0.013027    -0.015142     0.029848  \n",
              "...           ...          ...          ...          ...          ...  \n",
              "2085    -0.004628     0.012152     0.015378    -0.010386    -0.023502  \n",
              "2086    -0.001050     0.021730    -0.008739     0.003577     0.022280  \n",
              "2087     0.035619    -0.011251    -0.002966    -0.022544     0.019264  \n",
              "2088     0.003324     0.014035     0.022011    -0.023579     0.023065  \n",
              "2089    -0.013828     0.020227     0.028090    -0.013999     0.027782  \n",
              "\n",
              "[2090 rows x 12 columns]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "6e307d5d",
      "metadata": {
        "id": "6e307d5d"
      },
      "outputs": [],
      "source": [
        "sequence_length = 12\n",
        "prediction_length = 5\n",
        "sequenced_data = StockDataset(df_app,sequence_length,prediction_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "O0kSYJX-1OWO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "O0kSYJX-1OWO",
        "outputId": "60b2937a-62fd-4820-ebf6-d34576124e24"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "__main__.StockDataset"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(sequenced_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "df600f0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df600f0a",
        "outputId": "e7800da1-2f9f-4c5d-b428-18daecfb35a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Size: 1658/2073\n",
            "Validation Size: 207/2073\n",
            "Test Size: 208/2073\n"
          ]
        }
      ],
      "source": [
        "#Splitting the data to 80% Training, 10% Validaiton and 10% Testing\n",
        "split=0.8\n",
        "#Splitting the indices of the sequences, so as to maintain order of time series\n",
        "indices = list(range(len(sequenced_data)))\n",
        "\n",
        "#splitting the indices according to the decided split\n",
        "train_indices, test_indices = train_test_split(indices, train_size=split, shuffle=False)\n",
        "val_indices, test_indices = train_test_split(test_indices, train_size=0.5, shuffle=False)\n",
        "\n",
        "# Create the training , validation and test datasets\n",
        "train_dataset = torch.utils.data.Subset(sequenced_data, train_indices)\n",
        "val_dataset= torch.utils.data.Subset(sequenced_data, val_indices)\n",
        "test_dataset = torch.utils.data.Subset(sequenced_data, test_indices)\n",
        "train_size=len(train_dataset)\n",
        "test_size=len(val_dataset)\n",
        "val_size=len(test_dataset)\n",
        "print(f\"Train Size: {len(train_dataset)}/{len(sequenced_data)}\")\n",
        "print(f\"Validation Size: {len(val_dataset)}/{len(sequenced_data)}\")\n",
        "print(f\"Test Size: {len(test_dataset)}/{len(sequenced_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "82ff4681",
      "metadata": {
        "id": "82ff4681"
      },
      "outputs": [],
      "source": [
        "train_dataloader=DataLoader(train_dataset,batch_size=16,shuffle=False)\n",
        "val_dataloader=DataLoader(val_dataset,batch_size=16,shuffle=False)\n",
        "test_dataloader=DataLoader(test_dataset,batch_size=16,shuffle=False)\n",
        "entire_dataloader=DataLoader(sequenced_data,batch_size=16,shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38372a18",
      "metadata": {
        "id": "38372a18"
      },
      "source": [
        "### Model:\n",
        "#### Diffusion Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "967a6ab2",
      "metadata": {
        "id": "967a6ab2"
      },
      "outputs": [],
      "source": [
        "## Defining the class for diffusion module which keeps on adding gaussian noise with a fixed variance schedule to both input as wel as output\n",
        "class DiffusionProcess(nn.Module):\n",
        "    def __init__(self, num_diff_steps, vae, beta_start, beta_end, scale):\n",
        "        super().__init__()\n",
        "        to_torch = partial(torch.tensor, dtype = torch.float32)\n",
        "        ## Initializing variables like number of time stamps, the Hierarchial VAE to make predictions, start and end values\n",
        "        ## for beta, which governs the variance schedule\n",
        "        self.num_diff_steps = num_diff_steps\n",
        "        self.vae = vae\n",
        "        self.beta_start = beta_start\n",
        "        self.beta_end = beta_end\n",
        "        ## Defining a linearly varying variance schedule for the conditional noise at every timestamp\n",
        "        betas = np.linspace(beta_start, beta_end,  num_diff_steps, dtype = np.float32)\n",
        "\n",
        "        ## Performing reparametrization to calculate output at time t directly using x_start\n",
        "        alphas = 1 - betas\n",
        "        alphas_target = 1 - betas*scale\n",
        "        ## Computing the cumulative product for the input as well as output noise schedule\n",
        "        alphas_cumprod = np.cumprod(alphas, axis = 0)\n",
        "        alphas_target_cumprod = np.cumprod(alphas_target, axis = 0)\n",
        "\n",
        "        ## Converting all the computed quantities to tensors and detaching them from the computation graph (setting requires_grad to False)\n",
        "        betas = torch.tensor(betas, requires_grad = False)\n",
        "        alphas_cumprod = torch.tensor(alphas_cumprod, requires_grad = False)\n",
        "        alphas_target_cumprod = torch.tensor(alphas_target_cumprod, requires_grad = False)\n",
        "\n",
        "        ## Computing scaling factors for mean and variance respectively\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod).detach().requires_grad_(False)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod).detach().requires_grad_(False)\n",
        "        self.sqrt_alphas_target_cumprod = torch.sqrt(alphas_target_cumprod).detach().requires_grad_(False)\n",
        "        self.sqrt_one_minus_alphas_target_cumprod = torch.sqrt(1 - alphas_target_cumprod).detach().requires_grad_(False)\n",
        "\n",
        "    ## Defining the forward pass\n",
        "    def diffuse(self, x_start, y_target, timestamp):\n",
        "        ## Generating a random noise vector sampled from a standard normal of the size x_start and y_target respectively\n",
        "        noise = torch.randn_like(x_start)\n",
        "        noise_target = torch.randn_like(y_target)\n",
        "\n",
        "        ## Computing the sampled value using the reparametrization trick and using that to calculate x_noisy and y_noisy\n",
        "        x_noisy = self.sqrt_alphas_cumprod[timestamp - 1]*x_start + self.sqrt_one_minus_alphas_cumprod[timestamp - 1]*noise\n",
        "        y_noisy = self.sqrt_alphas_target_cumprod[timestamp - 1]*y_target + self.sqrt_one_minus_alphas_target_cumprod[timestamp - 1]*noise_target\n",
        "\n",
        "        ## Performing a forward pass through the Hierarchial VAE to generate noisy predictions\n",
        "        output = self.vae(x_noisy)\n",
        "        return output, y_noisy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0d6382f",
      "metadata": {
        "id": "b0d6382f"
      },
      "source": [
        "### Blocks required for Encoder and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "b1738576",
      "metadata": {
        "id": "b1738576"
      },
      "outputs": [],
      "source": [
        "import torch.nn.init as init\n",
        "\n",
        "## Initializing weights using Xavier Initialization\n",
        "def init_weights(layer):\n",
        "    init.xavier_uniform_(layer.weight)\n",
        "    layer_name=layer._class.name_\n",
        "    if layer.find(\"Conv\")!=-1:\n",
        "        layer.weight.data.normal_(0.0,0.25)\n",
        "    elif layer.find(\"BatchNorm\")!=-1:\n",
        "        layer.weight.data.normal(1.00,0.25)\n",
        "        layer.bias.data.fill_(0.00)\n",
        "\n",
        "## Defining a custom Conv2D class with the padding size such that the input size and output size remain the same\n",
        "class Conv2D(nn.Module):\n",
        "    def __init__(self,input_dim,output_dim,kernel_size,stride):\n",
        "        super(Conv2D,self).__init__()\n",
        "        ## Required padding size = kernel_size - 1/2\n",
        "        padding=int((kernel_size-1)/2)\n",
        "        self.layer=nn.Conv2d(input_dim,output_dim,kernel_size,stride=stride,padding=padding,bias=True)\n",
        "    ## Performing the forward pass\n",
        "    def forward(self, input):\n",
        "        # Print the shape of the input tensor\n",
        "        print(f\"Input Tensor Shape conv2d: {input.shape}\")\n",
        "        \n",
        "        # Perform the convolution\n",
        "        output = self.layer(input)\n",
        "        \n",
        "        # Print the shape of the output tensor\n",
        "        print(f\"Output Tensor Shape: {output.shape}\")\n",
        "        \n",
        "        return output\n",
        "\n",
        "## Defining the module for Swish Activation or Sigmoid Linear Unit\n",
        "class Swish(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Swish,self).__init__()\n",
        "        self.layer=nn.SiLU()\n",
        "    def forward(self,input):\n",
        "        return self.layer(input)\n",
        "\n",
        "## Performing Batch Normalization by inherting it from torch.nn\n",
        "class BatchNorm(nn.Module):\n",
        "    def __init__(self,batch_dim,size):\n",
        "        super(BatchNorm,self).__init__()\n",
        "        ## Equivalent to BatchNorm as first dimension is batch_size\n",
        "        self.layer=nn.LayerNorm([batch_dim,size,size])\n",
        "\n",
        "    def forward(self,input):\n",
        "        return self.layer(input)\n",
        "        \n",
        "class SE(nn.Module):\n",
        "    def __init__(self,channels_in,channels_out):\n",
        "        super(SE,self).__init__()\n",
        "        ## Defining number of units to be compressed into\n",
        "        num_hidden=max(channels_out//16,4)\n",
        "        \n",
        "        ## Defining the network which compresses and expands to focus on features rather than noise\n",
        "        ## 2 networks req as 2 different input output dimensions are present in the Hierarchial VAE\n",
        "        self.se=nn.Sequential(nn.Linear(1,num_hidden),nn.ReLU(inplace=True),\n",
        "                                nn.Linear(num_hidden, 144), nn.Sigmoid())\n",
        "        self.se2=nn.Sequential(nn.Linear(1,num_hidden),nn.ReLU(inplace=True),\n",
        "                                nn.Linear(num_hidden, 36), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Print the shape of the input tensor\n",
        "        print(f\"Input Tensor Shape se: {input.shape}\")\n",
        "        \n",
        "        # Getting compressed vector\n",
        "        se = torch.mean(input, dim=[1, 2])\n",
        "        # Print the shape after mean operation\n",
        "        print(f\"Shape after mean operation: {se.shape}\")\n",
        "        \n",
        "        # Flattening out the layer\n",
        "        se = se.view(se.size(0), -1)\n",
        "        # Print the shape after flattening\n",
        "        print(f\"Shape after flattening: {se.shape}\")\n",
        "        \n",
        "        # Apply the appropriate SE network based on the number of input channels\n",
        "        if input.size(1) == 12:\n",
        "            se = self.se(se)\n",
        "            se = se.view(se.size(0), 12, 12)\n",
        "            # Print the shape after applying self.se and reshaping\n",
        "            print(f\"Shape after self.se and reshaping: {se.shape}\")\n",
        "        else:\n",
        "            se = self.se2(se)\n",
        "            se = se.view(se.size(0), 6, 6)\n",
        "            # Print the shape after applying self.se2 and reshaping\n",
        "            print(f\"Shape after self.se2 and reshaping: {se.shape}\")\n",
        "        \n",
        "        # Return the scaled input\n",
        "        output = input * se\n",
        "        # Print the shape of the output tensor\n",
        "        print(f\"Output Tensor Shape: {output.shape}\")\n",
        "        \n",
        "        return output\n",
        "\n",
        "## Performing pooling for downsampling using nn.AvgPool2D and using a kernel of size 2 to ensure that output size is halved\n",
        "class Pooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Pooling,self).__init__()\n",
        "        ## Using a 2x2 kernel and a stride of 2 in both directions\n",
        "        self.mean_pool = nn.AvgPool2d(kernel_size=(2, 2),padding=0,stride=(2,2))\n",
        "    def forward(self,input):\n",
        "        return self.mean_pool(input)\n",
        "\n",
        "## Defining a class to compute square of a quantity\n",
        "class Square(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Square,self).__init__()\n",
        "        pass\n",
        "    def forward(self,input):\n",
        "        return input**2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19fd9282",
      "metadata": {
        "id": "19fd9282"
      },
      "source": [
        "### Residual Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "5c12967f",
      "metadata": {
        "id": "5c12967f"
      },
      "outputs": [],
      "source": [
        "## Defining the encoder block to be used in Hierarchial VAE to convert to input into its latent space representation\n",
        "class Encoder_Block(nn.Module):\n",
        "    def __init__(self,input_dim,size,output_dim):\n",
        "        super().__init__()\n",
        "        ## Initializing the in and out dimensions of the conv layers and SE block\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.size = size\n",
        "        ## Defining the encoder layers i.e 2 Conv2D layers followed by Batch Normalization, a Conv2D layer of kernel size 1 and Squeeze and excitation at the end\n",
        "        self.seq=nn.Sequential(Conv2D(input_dim,input_dim,kernel_size=5,stride=1),\n",
        "                               Conv2D(input_dim,input_dim,kernel_size=1,stride=1),\n",
        "                               BatchNorm(input_dim,size),Swish(),\n",
        "                               Conv2D(input_dim,input_dim,kernel_size=3,stride=1),\n",
        "                               SE(input_dim,output_dim))\n",
        "    def forward(self,input):\n",
        "        ## Computing the final output as the sum of scaled encoded output and original input (result of skip connection i.e. residual encoder)\n",
        "        return input +0.1*self.seq(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae8b9d6a",
      "metadata": {
        "id": "ae8b9d6a"
      },
      "source": [
        "### Residual Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "902bd093",
      "metadata": {
        "id": "902bd093"
      },
      "outputs": [],
      "source": [
        "## Defining the decoder to be used in Hierarchial VAE to convert from latent space representations to noisy outputs\n",
        "class Decoder_Block(nn.Module):\n",
        "    def __init__(self,dim,size,output_dim):\n",
        "        super().__init__()\n",
        "        ## Defining the decoder net which comprises of Conv2D layers, BatchNorm and SE Blocks\n",
        "        ## We ensure that the dimension of the input and output stays the same at all instants as down/up sampling is done in a separate block\n",
        "        self.seq = nn.Sequential(\n",
        "            BatchNorm(dim,size),\n",
        "            Conv2D(dim,dim,kernel_size=1,stride=1),\n",
        "            BatchNorm(dim,size), Swish(),\n",
        "            Conv2D(dim,dim, kernel_size=5, stride=1),\n",
        "            BatchNorm(dim,size), Swish(),\n",
        "            Conv2D(dim, dim, kernel_size=1, stride = 1),\n",
        "            BatchNorm(dim,size),\n",
        "            ## SE Block just compresses and expands which allows it to ignore noise and focus on actual indicators\n",
        "            SE(dim,output_dim))\n",
        "    ## Computing the final output similar to encoder taking into account the skip connection\n",
        "    def forward(self,input):\n",
        "        return input+0.1*self.seq(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13cae1c3",
      "metadata": {
        "id": "13cae1c3"
      },
      "source": [
        "### Hierarchial Variational Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "53dfd5e7",
      "metadata": {
        "id": "53dfd5e7"
      },
      "outputs": [],
      "source": [
        "## Defining the class for the Hierarchial VAE which takes as input various hyperparameters and the classes for encoder and decoder blocks\n",
        "class HierarchialVAE(nn.Module):\n",
        "    def __init__(self, Encoder_Block, Decoder_Block, latent_dim2, latent_dim1, feature_size2, \n",
        "                 feature_size1, hidden_size, pred_length, num_features, seq_length, batch_size):\n",
        "        super().__init__()\n",
        "        ## Initializing the encoder at the beginning when x_start has 12 features\n",
        "        self.Encoder1 = Encoder_Block(input_dim = batch_size, output_dim = batch_size, size = 12)\n",
        "        ## Initializing the encoder reqd after downsampling when input has 6 features \n",
        "        self.Encoder2 = Encoder_Block(input_dim = batch_size, output_dim = batch_size, size = 6)\n",
        "        ## Initializing the decoder reqd after upsampling which gives y_noisy at the output\n",
        "        self.Decoder1 = Decoder_Block(dim = batch_size,size = 12,output_dim = batch_size)\n",
        "        ## Initializing the first decoder which obtains an input of size batchx6x6\n",
        "        self.Decoder2 = Decoder_Block(dim = batch_size,size = 6,output_dim = batch_size)\n",
        "        \n",
        "        ## Initializing dimensions of both latent vectors, feature size of both the intermediate feature maps \n",
        "        self.latent_dim2 = latent_dim2\n",
        "        self.latent_dim1 = latent_dim1\n",
        "        self.feature_size2 = feature_size2\n",
        "        self.feature_size1 = feature_size1\n",
        "        ## Initializing the initial hidden state with a tensor of zeros with dimension equal to that of the final latent vector\n",
        "        self.hidden_size = hidden_size\n",
        "        self.hidden_state = torch.zeros(self.latent_dim1)\n",
        "        ## Initializing batch_size\n",
        "        self.batch_size= batch_size\n",
        "        \n",
        "        ## Defining the upsampling blocks required at 2 different stages in the entire network (2 networks reqd as size of input feature map varies throughout the network)\n",
        "        self.upsample1 = nn.Upsample(size=(6, 6), mode='bilinear', align_corners=False)\n",
        "        self.upsample2 = nn.Upsample(size=(12, 12), mode='bilinear', align_corners=False)\n",
        "        ## Defining linear layers that map flattened feature maps to latent space dimensions and vice versa\n",
        "        self.fc12 = nn.Linear(feature_size2,2*latent_dim2)\n",
        "        self.fc11 = nn.Linear(feature_size1,2*latent_dim1)\n",
        "        self.fc22 = nn.Linear(latent_dim2, feature_size2)\n",
        "        self.fc21 = nn.Linear(latent_dim1, feature_size1)\n",
        "        ## Defining pooling layer for downsampling\n",
        "        self.mean_pool = nn.AvgPool2d(kernel_size=(2, 2),padding=0,stride=(2,2))\n",
        "        ## The final linear layer which maps the VAE output to the output dimension\n",
        "        self.fc_final = nn.Linear(num_features*seq_length, pred_length)\n",
        "\n",
        "        \n",
        "    \n",
        "    def forward(self,x_start):\n",
        "        print(\"Vae x_start\")\n",
        "        print(x_start.shape)\n",
        "        ## We pass the input through two encoder blocks followed by pooling which reduces the feature map size to 6x6\n",
        "        out = self.Encoder1(x_start)\n",
        "        print(out.shape)\n",
        "        print(\"end1\")\n",
        "        out = self.Encoder1(out)\n",
        "        print(out.shape)\n",
        "        print(\"end1\")\n",
        "        out = self.mean_pool(out)\n",
        "        print(out.shape)\n",
        "        print(\"end1\")\n",
        "        ## Reshaping the feature map and storing as it is required for sampling \n",
        "        feature_map2 = out.view(out.size(0),6,6)\n",
        "        print(feature_map2.shape)\n",
        "        print(\"end1\")\n",
        "        ## Encoding and Pooling the output once again which reduces the feature map size to 3x3 \n",
        "        out = self.Encoder2(out)\n",
        "        print(out.shape)\n",
        "        print(\"end1\")\n",
        "        out = self.mean_pool(out)\n",
        "        print(out.shape)\n",
        "        print(\"end1\")\n",
        "        ## Flattening the final feature map and passing it through the linear layer which maps it to a latent vector of \n",
        "        ## dimension 4 (latent vector is dimension 2, but we predict both the mean and variances)\n",
        "        feature_map1 = out.view(out.size(0),-1)\n",
        "        print(feature_map1.shape)\n",
        "        print(\"end1\")\n",
        "        z1 = self.fc11(feature_map1)\n",
        "        print(z1.shape)\n",
        "        print(\"end1\")\n",
        "        ## Randomly sampling noise from a standard normal\n",
        "        noise1 = torch.randn((out.size(0),self.latent_dim1))\n",
        "        print(noise1.shape)\n",
        "        ## Applying the reparametrization trick to get the sampled value\n",
        "        sampled_z1 = self.reparametrize(noise1,z1)\n",
        "        print(sampled_z1.shape)\n",
        "        print(\"end1\")\n",
        "        ## Adding the initial hidden vector to the sampled output and converting it back to 3x3 feature map using a linear layer\n",
        "        out = sampled_z1 + self.hidden_state\n",
        "        print(out.shape)\n",
        "        print(\"end2\")\n",
        "        out = self.fc21(out)\n",
        "        print(out.shape)\n",
        "        print(\"end2\")\n",
        "        out = out.view(out.size(0),3,3)\n",
        "        print(out.shape)\n",
        "        print(\"end2\")\n",
        "        ## Upsampling to dimension 6x6\n",
        "        out = self.upsample1(out.unsqueeze(0)).squeeze(0)\n",
        "        print(out.shape)\n",
        "        print(\"end2\")\n",
        "        ## Passing it through the decoder and combining it with feature map 2 to sample from the 2nd latent vector\n",
        "        out = self.Decoder2(out)\n",
        "        print(out.shape)\n",
        "        print(\"end2\")\n",
        "        ## Maps to a dimension of 10 after flattening the vector which means means and variances of a latent vector of dim = 5\n",
        "        z_decoder = (feature_map2 + out).view(out.size(0),-1)\n",
        "        print(z_decoder.shape)\n",
        "        print(\"end2\")\n",
        "        z2 = self.fc12(z_decoder)\n",
        "        print(z2.shape)\n",
        "        print(\"end2\")\n",
        "        ## In a similar fashion, we get the sampled value from z2\n",
        "        noise2 = torch.randn((out.size(0),self.latent_dim2))\n",
        "        sampled_z2 = self.reparametrize(noise2,z2)\n",
        "        ## We convert it back to dim = 36 using a linear layer followed by reshaping it to 6x6\n",
        "        z2_upsampled = self.fc22(sampled_z2).view(out.size(0),6,6)\n",
        "        ## Upsampling to the original dimension of 12x12\n",
        "        out = out + z2_upsampled\n",
        "        out = self.upsample2(out.unsqueeze(0)).squeeze(0)\n",
        "        out = self.Decoder1(out)\n",
        "        out = self.Decoder1(out)\n",
        "        ## Passing it through the final linear layer to map it to the shape of output\n",
        "        out = self.fc_final(out.view(out.size(0),-1))\n",
        "        print(\"end\")\n",
        "        # raise RuntimeError(\"Stop before return out\")\n",
        "        return out\n",
        "        \n",
        "    def reparametrize(self,noise,z):\n",
        "        ## Getting the batch_size\n",
        "        zsize=int(z.size(1))\n",
        "        ## Initializing tensors for mean and variances\n",
        "        sampled_z = torch.zeros((noise.size(0),zsize//2))\n",
        "        mu=torch.zeros((noise.size(0),zsize//2))\n",
        "        sig=torch.zeros((noise.size(0),zsize//2))\n",
        "        for i in range(0,zsize//2):\n",
        "            mu[:,i]=z[:,i]\n",
        "            sig[:,i]=z[:,zsize//2+i]\n",
        "            ## Computing the sampled value\n",
        "            sampled_z[:,i]=mu[:,i] + noise[:,i]*sig[:,i]\n",
        "        return sampled_z\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "b5424636",
      "metadata": {
        "id": "b5424636"
      },
      "outputs": [],
      "source": [
        "## Defining the network for denoising score matching\n",
        "class Denoise_net(nn.Module):\n",
        "    def __init__(self,in_channels,dim,size,number=5):\n",
        "        super().__init__()\n",
        "        ## 2*number is number of diffusion samples used for denoise calculation\n",
        "        ## Initializing the input dimension (actually prediction length in this case)\n",
        "        hw = size\n",
        "        self.dim=dim\n",
        "        ## Number of input channels (batched mapping)\n",
        "        self.channels=in_channels\n",
        "        ## Defining the network for energy calculation\n",
        "        self.conv=Conv2D(self.channels,dim,3,1)\n",
        "        self.conv1=Conv2D(dim,dim,3,1)\n",
        "        self.relu1=nn.ELU()\n",
        "        self.pool1=Pooling()\n",
        "        self.conv2=Conv2D(dim,dim,3,1)\n",
        "        self.relu2=nn.ELU()\n",
        "        self.conv3=Conv2D(dim,dim,3,1)\n",
        "        self.relu3=nn.ELU()\n",
        "        ## Getting interaction energy and self energy component field terms\n",
        "        self.f1=nn.Linear((int(hw/2)*number),1)\n",
        "        self.f2=nn.Linear((int(hw/2)*number),1)\n",
        "        self.fq=nn.Linear((int(hw/2)*number),1)\n",
        "        self.square=Square()\n",
        "\n",
        "    def forward(self,input):\n",
        "        output=self.conv(input)\n",
        "        output1=self.conv1(output)\n",
        "        output2=self.relu1(output1)\n",
        "        ## Resnet type output computation for stable gradient flow\n",
        "        output2=output2+output1\n",
        "        ## Pooling to increase the receptive field\n",
        "        output3=self.pool1(output2)\n",
        "        output4=self.conv2(output3)\n",
        "        output5=self.relu2(output4)\n",
        "        output5=output5+output4\n",
        "        output7=self.conv3(output5)\n",
        "        output8=self.relu3(output7)\n",
        "        l1=self.f1(output8.view(input.size(0),-1))\n",
        "        l2=self.f2(output8.view(input.size(0),-1))\n",
        "        lq=self.fq(self.square(output8.view(input.size(0),-1)))\n",
        "        ## Getting gradient of energy term per sample (gradient of energy term is what we are concerned with)\n",
        "        out=l1*l2 +lq\n",
        "        out=out.view(-1)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "7c78b365",
      "metadata": {
        "id": "7c78b365"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "## Initializing the VAE and Diffusion block with appropriate hyperparameters\n",
        "VAE = HierarchialVAE(Encoder_Block = Encoder_Block, Decoder_Block = Decoder_Block , latent_dim2 = 5, latent_dim1 = 2, feature_size2 = 36,\n",
        "                 feature_size1 = 9, hidden_size = 2, pred_length = 5, num_features = 12, seq_length = 12, batch_size=16)\n",
        "Diffusion_Process = DiffusionProcess(num_diff_steps = 10, vae = VAE, beta_start = 0.01, beta_end = 0.1, scale = 0.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "1884c4c3",
      "metadata": {
        "id": "1884c4c3"
      },
      "outputs": [],
      "source": [
        "## Initializing the Denoising network with appropriate hyperparameters\n",
        "Denoise_Net = Denoise_net(in_channels = 16,dim = 16, size = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c8ac8529",
      "metadata": {
        "id": "c8ac8529"
      },
      "outputs": [],
      "source": [
        "## Defining the MSE Loss and optimizers for parameters of the VAE and denoising net\n",
        "from torch.optim.lr_scheduler import StepLR,CosineAnnealingLR\n",
        "criterion=nn.MSELoss()\n",
        "optimizer1=optim.Adam(VAE.parameters(),lr=3e-3)\n",
        "optimizer2=optim.Adam(Denoise_Net.parameters(),lr=3e-3)\n",
        "## Using Step learning rate scheduler for both the optimizers to ensure stable convergence\n",
        "scheduler1= StepLR(optimizer1, step_size=2, gamma=0.5)\n",
        "scheduler2 = StepLR(optimizer2, step_size=2, gamma=0.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc67ebaa",
      "metadata": {
        "id": "fc67ebaa"
      },
      "outputs": [],
      "source": [
        "## Defining the training loop with number of epochs, VAE and Dnet's and dataloaders as the inputs\n",
        "def train(epochs,train_dataloader,val_dataloader,VAE,dnet,num_diff_steps):\n",
        "\n",
        "    ## List for accumulating training and validation losses\n",
        "    train_loss=[]\n",
        "    val_loss=[]\n",
        "\n",
        "    ## Iterating over number of epochs\n",
        "    for epoch in range(0,epochs):\n",
        "\n",
        "        total_loss=0\n",
        "        ## Setting the both the models into training mode\n",
        "        VAE.train()\n",
        "        dnet.train()\n",
        "        for i,(x,y) in enumerate(train_dataloader):\n",
        "            if(x.size(0)!=16):\n",
        "                break\n",
        "            ## Initializing the VAE and diffusion outputs\n",
        "            vae_out = torch.zeros((y.size(0), y.size(1),num_diff_steps))\n",
        "            diff_out = torch.zeros((y.size(0), y.size(1),num_diff_steps))\n",
        "\n",
        "            ## For number of diffusion timestamps..\n",
        "            for time in range(1,num_diff_steps + 1):\n",
        "                ## We compute the diffused target as well as target predicted by the VAE\n",
        "                output, y_noisy = Diffusion_Process.diffuse(x,y,time)\n",
        "                vae_out[:,:,time-1] = output\n",
        "                diff_out[:,:,time-1] = y_noisy\n",
        "            ## To get a approximate distribution of the outputs of the VAE and those produced by the diffusion net by adding noise\n",
        "            ## we use the mean and variances of all outputs of all timestamps (assuming the distribution to be normal)\n",
        "            mean_vae = torch.mean(vae_out, dim = 2)\n",
        "            mean_diff = torch.mean(diff_out, dim = 2)\n",
        "            var_vae = torch.std(vae_out, dim = 2)\n",
        "            var_diff = torch.std(diff_out, dim = 2)\n",
        "            optimizer1.zero_grad()\n",
        "            optimizer2.zero_grad()\n",
        "            ## Computing the MSE loss between mean values of both the outputs\n",
        "            mse_loss = criterion(mean_vae, mean_diff)\n",
        "            ## Computing the KL divergence between both the distributions\n",
        "            ## We used the standard formula for KL divergence between 2 multivariate gaussians\n",
        "            term1 = (mean_vae - mean_diff) / var_diff\n",
        "            term2 = var_vae / var_diff\n",
        "            kl_loss =  0.5 * ((term1 * term1).sum() + (term2 * term2).sum()) - 40 - torch.log(term2).sum()\n",
        "            kl_loss = kl_loss.sum()\n",
        "\n",
        "            ran=torch.randint(low=1,high=num_diff_steps + 1,size=(1,))\n",
        "            y_nn=vae_out[:,:,:]\n",
        "\n",
        "            E = Denoise_Net(y_nn).sum()\n",
        "            grad_x = torch.autograd.grad(E, y_nn, create_graph=True)[0]\n",
        "            dsm_loss = torch.mean(torch.sum((y.unsqueeze(2)-y_nn+grad_x*1)**2, [0,1,2])).float()\n",
        "            ## Combining all the 3 losses with appropriate weights which are hyperparameters\n",
        "            loss = 4*mse_loss+0.01*kl_loss+ 0.1*dsm_loss\n",
        "            total_loss+=loss\n",
        "            ## Performing backpropogation and gradient descent\n",
        "            loss.backward()\n",
        "            optimizer1.step()\n",
        "            optimizer2.step()\n",
        "\n",
        "        ## Updating the learning rate of both the optimizers according to the specified schedule\n",
        "        scheduler1.step()\n",
        "        scheduler2.step()\n",
        "\n",
        "        totalval_loss=0\n",
        "\n",
        "        ## Setting the model to evaluation mode\n",
        "        VAE.eval()\n",
        "        dnet.eval()\n",
        "        for i,(x,y) in enumerate(val_dataloader):\n",
        "            if(x.size(0)!=16):\n",
        "                break\n",
        "            ## Initializing the VAE and diffusion outputs\n",
        "            vae_out = torch.zeros((y.size(0), y.size(1),num_diff_steps))\n",
        "            diff_out = torch.zeros((y.size(0), y.size(1),num_diff_steps))\n",
        "\n",
        "            ## Performing forward pass and computing the losses in a fashion similar to training\n",
        "            for time in range(1,num_diff_steps + 1):\n",
        "                output, y_noisy = Diffusion_Process.diffuse(x,y,time)\n",
        "                vae_out[:,:,time-1] = output\n",
        "                diff_out[:,:,time-1] = y_noisy\n",
        "            mean_vae = torch.mean(vae_out, dim = 2)\n",
        "            mean_diff = torch.mean(diff_out, dim = 2)\n",
        "            var_vae = torch.std(vae_out, dim = 2)\n",
        "            var_diff = torch.std(diff_out, dim = 2)\n",
        "            mse_loss = criterion(mean_vae, mean_diff)\n",
        "            term1 = (mean_vae - mean_diff) / var_diff\n",
        "            term2 = var_vae / var_diff\n",
        "            kl_loss =  0.5 * ((term1 * term1).sum() + (term2 * term2).sum()) - 40 - torch.log(term2).sum()\n",
        "            kl_loss = kl_loss.sum()\n",
        "            ran=torch.randint(low=1,high=num_diff_steps + 1,size=(1,))\n",
        "            y_nn=vae_out[:,:,:]\n",
        "            E = Denoise_Net(y_nn).sum()\n",
        "            grad_x = torch.autograd.grad(E, y_nn, create_graph=True)[0]\n",
        "            dsm_loss = torch.mean(torch.sum((y.unsqueeze(2)-y_nn+grad_x*1)**2, [0,1,2])).float()\n",
        "            ## Computing the total validation loss\n",
        "            valloss = 4*mse_loss+0.01*kl_loss+ 0.1*dsm_loss\n",
        "            totalval_loss+=valloss\n",
        "\n",
        "        ## Averaging out the training loss over all batches and printing the losses after every epoch\n",
        "        train_loss.append(total_loss/(len(train_dataloader)))\n",
        "        val_loss.append(totalval_loss/(len(val_dataloader)))\n",
        "        print(f\"Epoch: {epoch+1}\")\n",
        "        print(f\"Training :: Loss:{train_loss[epoch]}\")\n",
        "        print(f\"Validation :: Loss:{val_loss[epoch]}\")\n",
        "\n",
        "    return train_loss,val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f59b1265",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "f59b1265",
        "outputId": "35202d1d-96b5-4ebb-beaa-273497a63c11",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vae x_start\n",
            "torch.Size([16, 12, 12])\n",
            "Input Tensor Shape conv2d: torch.Size([16, 12, 12])\n",
            "Output Tensor Shape: torch.Size([16, 12, 12])\n",
            "Input Tensor Shape conv2d: torch.Size([16, 12, 12])\n",
            "Output Tensor Shape: torch.Size([16, 12, 12])\n",
            "Input Tensor Shape conv2d: torch.Size([16, 12, 12])\n",
            "Output Tensor Shape: torch.Size([16, 12, 12])\n",
            "Input Tensor Shape se: torch.Size([16, 12, 12])\n",
            "Shape after mean operation: torch.Size([16])\n",
            "Shape after flattening: torch.Size([16, 1])\n",
            "Shape after self.se and reshaping: torch.Size([16, 12, 12])\n",
            "Output Tensor Shape: torch.Size([16, 12, 12])\n",
            "torch.Size([16, 12, 12])\n",
            "end1\n",
            "Input Tensor Shape conv2d: torch.Size([16, 12, 12])\n",
            "Output Tensor Shape: torch.Size([16, 12, 12])\n",
            "Input Tensor Shape conv2d: torch.Size([16, 12, 12])\n",
            "Output Tensor Shape: torch.Size([16, 12, 12])\n",
            "Input Tensor Shape conv2d: torch.Size([16, 12, 12])\n",
            "Output Tensor Shape: torch.Size([16, 12, 12])\n",
            "Input Tensor Shape se: torch.Size([16, 12, 12])\n",
            "Shape after mean operation: torch.Size([16])\n",
            "Shape after flattening: torch.Size([16, 1])\n",
            "Shape after self.se and reshaping: torch.Size([16, 12, 12])\n",
            "Output Tensor Shape: torch.Size([16, 12, 12])\n",
            "torch.Size([16, 12, 12])\n",
            "end1\n",
            "torch.Size([16, 6, 6])\n",
            "end1\n",
            "torch.Size([16, 6, 6])\n",
            "end1\n",
            "Input Tensor Shape conv2d: torch.Size([16, 6, 6])\n",
            "Output Tensor Shape: torch.Size([16, 6, 6])\n",
            "Input Tensor Shape conv2d: torch.Size([16, 6, 6])\n",
            "Output Tensor Shape: torch.Size([16, 6, 6])\n",
            "Input Tensor Shape conv2d: torch.Size([16, 6, 6])\n",
            "Output Tensor Shape: torch.Size([16, 6, 6])\n",
            "Input Tensor Shape se: torch.Size([16, 6, 6])\n",
            "Shape after mean operation: torch.Size([16])\n",
            "Shape after flattening: torch.Size([16, 1])\n",
            "Shape after self.se2 and reshaping: torch.Size([16, 6, 6])\n",
            "Output Tensor Shape: torch.Size([16, 6, 6])\n",
            "torch.Size([16, 6, 6])\n",
            "end1\n",
            "torch.Size([16, 3, 3])\n",
            "end1\n",
            "torch.Size([16, 9])\n",
            "end1\n",
            "torch.Size([16, 4])\n",
            "end1\n",
            "torch.Size([16, 2])\n",
            "torch.Size([16, 2])\n",
            "end1\n",
            "torch.Size([16, 2])\n",
            "end2\n",
            "torch.Size([16, 9])\n",
            "end2\n",
            "torch.Size([16, 3, 3])\n",
            "end2\n",
            "torch.Size([16, 6, 6])\n",
            "end2\n",
            "Input Tensor Shape conv2d: torch.Size([16, 6, 6])\n",
            "Output Tensor Shape: torch.Size([16, 6, 6])\n",
            "Input Tensor Shape conv2d: torch.Size([16, 6, 6])\n",
            "Output Tensor Shape: torch.Size([16, 6, 6])\n",
            "Input Tensor Shape conv2d: torch.Size([16, 6, 6])\n",
            "Output Tensor Shape: torch.Size([16, 6, 6])\n",
            "Input Tensor Shape se: torch.Size([16, 6, 6])\n",
            "Shape after mean operation: torch.Size([16])\n",
            "Shape after flattening: torch.Size([16, 1])\n",
            "Shape after self.se2 and reshaping: torch.Size([16, 6, 6])\n",
            "Output Tensor Shape: torch.Size([16, 6, 6])\n",
            "torch.Size([16, 6, 6])\n",
            "end2\n",
            "torch.Size([16, 36])\n",
            "end2\n",
            "torch.Size([16, 10])\n",
            "end2\n",
            "Input Tensor Shape conv2d: torch.Size([16, 12, 12])\n",
            "Output Tensor Shape: torch.Size([16, 12, 12])\n",
            "Input Tensor Shape conv2d: torch.Size([16, 12, 12])\n",
            "Output Tensor Shape: torch.Size([16, 12, 12])\n",
            "Input Tensor Shape conv2d: torch.Size([16, 12, 12])\n",
            "Output Tensor Shape: torch.Size([16, 12, 12])\n",
            "Input Tensor Shape se: torch.Size([16, 12, 12])\n",
            "Shape after mean operation: torch.Size([16])\n",
            "Shape after flattening: torch.Size([16, 1])\n",
            "Shape after self.se and reshaping: torch.Size([16, 12, 12])\n",
            "Output Tensor Shape: torch.Size([16, 12, 12])\n",
            "Input Tensor Shape conv2d: torch.Size([16, 12, 12])\n",
            "Output Tensor Shape: torch.Size([16, 12, 12])\n",
            "Input Tensor Shape conv2d: torch.Size([16, 12, 12])\n",
            "Output Tensor Shape: torch.Size([16, 12, 12])\n",
            "Input Tensor Shape conv2d: torch.Size([16, 12, 12])\n",
            "Output Tensor Shape: torch.Size([16, 12, 12])\n",
            "Input Tensor Shape se: torch.Size([16, 12, 12])\n",
            "Shape after mean operation: torch.Size([16])\n",
            "Shape after flattening: torch.Size([16, 1])\n",
            "Shape after self.se and reshaping: torch.Size([16, 12, 12])\n",
            "Output Tensor Shape: torch.Size([16, 12, 12])\n",
            "end\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Stop before return out",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[151], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loss,val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                             \u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVAE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVAE\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdnet\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDenoise_Net\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_diff_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[150], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs, train_dataloader, val_dataloader, VAE, dnet, num_diff_steps)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m## For number of diffusion timestamps..\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m time \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,num_diff_steps \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m## We compute the diffused target as well as target predicted by the VAE\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     output, y_noisy \u001b[38;5;241m=\u001b[39m \u001b[43mDiffusion_Process\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffuse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     vae_out[:,:,time\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m     27\u001b[0m     diff_out[:,:,time\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m y_noisy\n",
            "Cell \u001b[0;32mIn[141], line 44\u001b[0m, in \u001b[0;36mDiffusionProcess.diffuse\u001b[0;34m(self, x_start, y_target, timestamp)\u001b[0m\n\u001b[1;32m     41\u001b[0m y_noisy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msqrt_alphas_target_cumprod[timestamp \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39my_target \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msqrt_one_minus_alphas_target_cumprod[timestamp \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39mnoise_target\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m## Performing a forward pass through the Hierarchial VAE to generate noisy predictions\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_noisy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, y_noisy\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[145], line 118\u001b[0m, in \u001b[0;36mHierarchialVAE.forward\u001b[0;34m(self, x_start)\u001b[0m\n\u001b[1;32m    116\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_final(out\u001b[38;5;241m.\u001b[39mview(out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m),\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStop before return out\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Stop before return out"
          ]
        }
      ],
      "source": [
        "train_loss,val_loss = train(epochs =20\n",
        "                             ,train_dataloader = train_dataloader, val_dataloader = val_dataloader, VAE = VAE,dnet = Denoise_Net, num_diff_steps = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "164992f0",
      "metadata": {
        "id": "164992f0"
      },
      "outputs": [],
      "source": [
        "#Detaching the losses in epochs so as to plot\n",
        "train_loss = [tensor.detach() for tensor in train_loss]\n",
        "val_loss = [tensor.detach() for tensor in val_loss]\n",
        "plt.figure(figsize=(11, 8))\n",
        "#Plotting the loss vs epochs graph\n",
        "plt.plot((np.arange(2,21,1)),train_loss[1:],label='Validation Loss')\n",
        "plt.plot((np.arange(2,21,1)),val_loss[1:],label='Training Loss')\n",
        "plt.title(\"Loss vs Epochs\",fontsize=16)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel(\"Loss (MSE+KL+DSM)\",fontsize=16)\n",
        "plt.legend()\n",
        "plt.xticks([2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])\n",
        "plt.grid()\n",
        "plt.show()\n",
        "#As can be seen the training loss keeps reducing and after certain epochs reaches a saturation point, so does the validation loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaf18d65",
      "metadata": {
        "id": "eaf18d65"
      },
      "outputs": [],
      "source": [
        "#We define testing function\n",
        "def test(test_dataloader,VAE,dnet,num_diff_steps):\n",
        "    #to calculate kl,mse,dsm loss seperately, and get the predicted and target sequences\n",
        "    totaltest_loss=0\n",
        "    totalmse_loss=0\n",
        "    totalkl_loss=0\n",
        "    totaldsm_loss=0\n",
        "    predicted_seq=[]\n",
        "    inp_seq=[]\n",
        "    target_seq=[]\n",
        "    for i,(x,y) in enumerate(test_dataloader):\n",
        "        if(x.size(0)!=16):\n",
        "            break\n",
        "        vae_out = torch.zeros((y.size(0), y.size(1),num_diff_steps))\n",
        "        diff_out = torch.zeros((y.size(0), y.size(1),num_diff_steps))\n",
        "\n",
        "        #Similar to the training loop\n",
        "        for time in range(1,num_diff_steps + 1):\n",
        "            output, y_noisy = Diffusion_Process.diffuse(x,y,time)\n",
        "            vae_out[:,:,time-1] = output\n",
        "            diff_out[:,:,time-1] = y_noisy\n",
        "        mean_vae = torch.mean(vae_out, dim = 2)\n",
        "        mean_diff = torch.mean(diff_out, dim = 2)\n",
        "        var_vae = torch.std(vae_out, dim = 2)\n",
        "        var_diff = torch.std(diff_out, dim = 2)\n",
        "        mse_loss = criterion(mean_vae, mean_diff)\n",
        "        term1 = (mean_vae - mean_diff) / var_diff\n",
        "        term2 = var_vae / var_diff\n",
        "        kl_loss =  0.5 * ((term1 * term1).sum() + (term2 * term2).sum()) - 40 - torch.log(term2).sum()\n",
        "        kl_loss = kl_loss.sum()\n",
        "        ran=torch.randint(low=1,high=num_diff_steps + 1,size=(1,))\n",
        "        y_nn=vae_out[:,:,:]\n",
        "        E = Denoise_Net(y_nn).sum()\n",
        "        grad_x = torch.autograd.grad(E, y_nn, create_graph=True)[0]\n",
        "        dsm_loss = torch.mean(torch.sum((y.unsqueeze(2)-y_nn+grad_x*1)**2, [0,1,2])).float()\n",
        "        testloss = 4*mse_loss+0.01*kl_loss+ 0.1*dsm_loss\n",
        "        totalmse_loss+=4*mse_loss\n",
        "        totalkl_loss+=0.01*kl_loss\n",
        "        totaldsm_loss+=0.1*dsm_loss\n",
        "        totaltest_loss+=testloss\n",
        "          #The predicted sequence will be nothing but the mean of vae - of the scaled gradient\n",
        "        inp_seq.append(x)\n",
        "        predicted_seq.append(mean_vae - 0.1*torch.mean(grad_x,dim=2))\n",
        "        target_seq.append(y)\n",
        "        #Computing and printing all of the losses\n",
        "    avg_test_loss=totaltest_loss/(len(test_dataloader))\n",
        "    avg_mse_loss=totalmse_loss/(len(test_dataloader))\n",
        "    avg_kl_loss=totalkl_loss/(len(test_dataloader))\n",
        "    avg_dsm_loss=totaldsm_loss/(len(test_dataloader))\n",
        "    print(f\"Test total Loss : {avg_test_loss}\")\n",
        "    print(f\"Test MSE Loss : {avg_mse_loss}\")\n",
        "    print(f\"Test KL Loss : {avg_kl_loss}\")\n",
        "    print(f\"Test DSM Loss : {avg_dsm_loss}\")\n",
        "    return inp_seq,predicted_seq,target_seq,avg_test_loss,avg_mse_loss,avg_kl_loss,avg_dsm_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ed0925f",
      "metadata": {
        "id": "7ed0925f"
      },
      "outputs": [],
      "source": [
        "#Testing the model based on losses on the testing dataset\n",
        "_,_,_,loss,mse,kl,dsm = test(test_dataloader,VAE,Denoise_net,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35745c8b",
      "metadata": {
        "id": "35745c8b"
      },
      "outputs": [],
      "source": [
        "#Predicting sequences for the entire dataset\n",
        "inp,pred,tar,_,_,_,_ = test(entire_dataloader,VAE,Denoise_net,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d203aac",
      "metadata": {
        "id": "9d203aac"
      },
      "outputs": [],
      "source": [
        "#Separting all of the sequences so as to remove the batch dimension\n",
        "target_sequence=[]\n",
        "for i in range (0,len(tar)):\n",
        "    for j in range(0,16):\n",
        "        target_sequence.append(tar[i][j])\n",
        "pred_sequence=[]\n",
        "for i in range(0,len(pred)):\n",
        "    for j in range(0,16):\n",
        "        pred_sequence.append(pred[i][j])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "315a1338",
      "metadata": {
        "id": "315a1338"
      },
      "outputs": [],
      "source": [
        "#Joining all of the sequences so as to create one list of predicted stock prices,\n",
        "#we got these for all the sequences,\n",
        "#we took every 5th predicted sequence to construct the plot\n",
        "tarcont_seq=[]\n",
        "for i in range(0,len(target_sequence)):\n",
        "    if(i%5==0):\n",
        "        tarcont_seq.append(target_sequence[i])\n",
        "tarcont_seq = [item.item() for sublist in tarcont_seq for item in sublist]\n",
        "predcont_seq=[]\n",
        "for i in range(0,len(pred_sequence)):\n",
        "    if(i%5==0):\n",
        "        predcont_seq.append(pred_sequence[i])\n",
        "predcont_seq = [item.item() for sublist in predcont_seq for item in sublist]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "379ebf8c",
      "metadata": {
        "id": "379ebf8c"
      },
      "outputs": [],
      "source": [
        "print(len(tarcont_seq))\n",
        "print(len(predcont_seq))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b29075cb",
      "metadata": {
        "id": "b29075cb"
      },
      "outputs": [],
      "source": [
        "#Denormalizing the sequences by multiplying with shifted Closes\n",
        "denorm_tar=apple_stock['Close'][6+5:2051+15]*tarcont_seq\n",
        "denorm_pred=apple_stock['Close'][6+5:2051+15]*predcont_seq\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23622aa0",
      "metadata": {
        "id": "23622aa0"
      },
      "outputs": [],
      "source": [
        "#Plotting Predicted vs actual for the training dataset\n",
        "plt.figure(figsize=(14,8))\n",
        "plt.plot(np.arange(0,len(denorm_tar[0:2048]),1),denorm_tar[0:2048],label='Target')\n",
        "plt.plot(np.arange(0,len(denorm_pred[0:2048]),1),denorm_pred[0:2048],label='Predicted')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.title('Actual vs Predicted Stock Price (Training Data)',fontsize=16)\n",
        "plt.xlabel('Days',fontsize=16)\n",
        "plt.ylabel('Stock Price (Denormalized)',fontsize=16)\n",
        "plt.plot()\n",
        "\n",
        "#Plotting Predicted vs actual for the validation dataset\n",
        "plt.figure(figsize=(14,8))\n",
        "plt.plot(np.arange(0,len(denorm_tar[2048:2048+256]),1),denorm_tar[2048:2048+256],label='Target')\n",
        "plt.plot(np.arange(0,len(denorm_pred[2048:2048+256]),1),denorm_pred[2048:2048+256],label='Predicted')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.title('Actual vs Predicted Stock Price (Validation Data)',fontsize=16)\n",
        "plt.xlabel('Days',fontsize=16)\n",
        "plt.ylabel('Stock Price (Denormalized)',fontsize=16)\n",
        "plt.plot()\n",
        "\n",
        "#Plotting Predicted vs actual for the testing dataset\n",
        "plt.figure(figsize=(14,8))\n",
        "plt.plot(np.arange(0,len(denorm_tar[2048+256:]),1),denorm_tar[2048+256:],label='Target')\n",
        "plt.plot(np.arange(0,len(denorm_pred[2048+256:]),1),denorm_pred[2048+256:],label='Predicted')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.title('Actual vs Predicted Stock Price (Test Data)',fontsize=16)\n",
        "plt.xlabel('Days',fontsize=16)\n",
        "plt.ylabel('Stock Price (Denormalized)',fontsize=16)\n",
        "plt.plot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7050ec9",
      "metadata": {
        "id": "f7050ec9"
      },
      "outputs": [],
      "source": [
        "# Assuming denorm_tar and denorm_pred are NumPy arrays containing target and predicted values\n",
        "\n",
        "# Calculate MAPE for training data\n",
        "mape_train = np.mean(np.abs((denorm_tar[0:2048] - denorm_pred[0:2048]) / denorm_tar[0:2048])) * 100\n",
        "print(f'MAPE for Training Data: {mape_train}%')\n",
        "\n",
        "# Calculate MAPE for validation data\n",
        "mape_val = np.mean(np.abs((denorm_tar[2048:2048+256] - denorm_pred[2048:2048+256]) / denorm_tar[2048:2048+256])) * 100\n",
        "print(f'MAPE for Validation Data: {mape_val}%')\n",
        "\n",
        "# Calculate MAPE for test data\n",
        "mape_test = np.mean(np.abs((denorm_tar[2048+256:] - denorm_pred[2048+256:]) / denorm_tar[2048+256:])) * 100\n",
        "print(f'MAPE for Test Data: {mape_test}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76ffbbaa",
      "metadata": {
        "id": "76ffbbaa"
      },
      "outputs": [],
      "source": [
        "#As can be seen our model predicts the stock prices for the next 5 days quiet appreciably!\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Assuming denorm_tar and denorm_pred are NumPy arrays containing target and predicted values\n",
        "\n",
        "# Calculate RMSE for training data\n",
        "rmse_train = np.sqrt(mean_squared_error(denorm_tar[0:2048], denorm_pred[0:2048]))\n",
        "print(f'RMSE for Training Data: {rmse_train}')\n",
        "\n",
        "# Calculate RMSE for validation data\n",
        "rmse_val = np.sqrt(mean_squared_error(denorm_tar[2048:2048+256], denorm_pred[2048:2048+256]))\n",
        "print(f'RMSE for Validation Data: {rmse_val}')\n",
        "\n",
        "# Calculate RMSE for test data\n",
        "rmse_test = np.sqrt(mean_squared_error(denorm_tar[2048+256:], denorm_pred[2048+256:]))\n",
        "print(f'RMSE for Test Data: {rmse_test}')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
